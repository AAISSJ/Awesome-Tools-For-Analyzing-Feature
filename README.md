# Tools For Analyzing Features 

## Features 
### 1. [SHAP](https://github.com/slundberg/shap)
- A game theoretic approach to explain the output of any machine learning model
- Functions
  - Natural language example (transformers)
  - Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)
  - Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)
  - Model agnostic example with KernelExplainer (explains any function)
### 2. 



## Model Visualization
### 1. [Transformer Interpretability Beyond Attention Visualization](https://github.com/hila-chefer/Transformer-Explainability)
- [CVPR 2021] Official PyTorch implementation for Transformer Interpretability Beyond Attention Visualization, a novel method to visualize classifications by Transformer based networks.
- Functions 
  - Reproducing results on ViT
    - Segmentation Results
    - Perturbation Results  
  - Reproducing results on BERT

### 2. [BertViz](https://github.com/jessevig/bertviz)
- BertViz: Visualize Attention in NLP Models (BERT, GPT2, BART, etc.)
- Functions
  - Head View : The head view visualizes attention for one or more attention heads in the same layer. 
  - Model View : The model view shows a bird's-eye view of attention across all layers and heads.
  - Neuron View : The neuron view visualizes individual neurons in the query and key vectors and shows how they are used to compute attention.

